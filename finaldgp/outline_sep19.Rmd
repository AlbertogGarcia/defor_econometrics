---
title: "deforestation econometrics results outline"
#author: "Alberto Garcia"
#date: "September 3, 2019"
#updated: "October 1, 2019"
output:
  pdf_document:
    number_sections: yes
    toc: no
fontsize: 11pt    
documentclass: article
geometry: margin=1in
header-includes: 
  \usepackage{sectsty}
  \usepackage{enumitem}
  \usepackage{comment}
  \usepackage{multirow,array}
  \usepackage{makecell}
  \usepackage{amsmath}
  \usepackage{amsfonts}
  \usepackage{amssymb} 
  \usepackage{graphicx}
  \usepackage{float}
  \usepackage{bbm}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
#binary fcns
source('quickmontey.R')
source('binary_coeffdist_fcn.R')
source('funcform.R')

#aggregated fcns
source('grid_scapegen.R')
source('outcome_comparison.R')
source('county_scapegen.R')
source('county_sims.R')
source('aggregation_method.R')
source('weightingarea.R')

source('xy_ATT.R')

```
\sectionfont{\fontsize{11}{11}\selectfont}
\subsectionfont{\fontsize{11}{11}\selectfont}

Define universal parameters
```{r parameters}
set.seed(930)

base_0 = .08
base_1 = .12
#trend = .00
trend = .02
ATT = -.05

std_a = .1
std_v = 0.2
years = 3
nobs = 10000
n = 100

cellsize = 10
ppoints = 50
std_p = 0.15
cpoints = 20

```


```{r generating beta parameters, results='hide', warning = FALSE}

std_av = (std_a^2+std_v^2)^.5
b0 = qnorm(base_0, mean = 0, sd = std_av)
b1 = qnorm(base_1, mean = 0, sd = std_av) - b0
b2 = qnorm(trend + base_0, mean = 0, sd = std_av) - b0
b3 = qnorm( pnorm(b0+b1+b2, mean = 0, sd = std_av) + ATT , mean = 0, sd = std_av) - (b0 + b1 + b2)

```
# DGP

## Setting initial parameters

The researcher sets the following four parameters:

\begin{align*}
baseline_0 &= E[y_{it}(0) |  t<t_0, D_i=0]\\
baseline_1 &= E[y_{it}(0) |  t<t_0, D_i=1]\\
trend &= E[y_{it}(0) |  t\geq t_0, D_i=0] - E[y_{it}(0) |  t<t_0, D_i=0]\\
ATT &= E[y_{it}(1) - y_{it}(0) |  t\geq t_0, D_i=1]\\
\end{align*}

Note the following constraints on the parameters:
\begin{align*}
E[y_{it}(0) |  t \geq t_0, D_i=0] \geq 0\\
E[y_{it}(1) |  t \geq t_0, D_i=1] \geq 0
\end{align*}

The initial DGP is now as follows. We define the latent variable, $y^*_{it}$,
\begin{align*}
y^*_{it} = \beta_0 + \beta_1 \mathbbm{1}\{  D_i = 1  \} +\beta_2 \mathbbm{1}\{  t \geq t_0  \} +\beta_3 \mathbbm{1}\{  D_i = 1  \} \mathbbm{1}\{  t \geq t_0  \} + \alpha_i +u_{it}
\end{align*}
, where the $\beta$ coefficients are derived from the four parameters assigned by the researcher. 

The treatment variable is generated according to $D_i \sim bernoulli(.5)$. The period in which the treatment is implemented is denoted $t_0$. The pixel specific parameter is generated according to $\alpha_i \sim N(0, \sigma^2_a)$ and the error term is generated according to $u_{it} \sim N(0, \sigma^2_u)$. 

The mapping from the latent to observed variable $y_{it}$ is

\[ y_{it} = \begin{cases} 
      1 & y^*_{it} > 0  \\
      0 & else
   \end{cases}
\]

## parameter to $\beta$ coefficient mapping

\begin{align*}
ATT =& E[y_{it}(1) - y_{it}(0) |  t\geq t_0, D_i=1] \\
=& E[ y_{it}(1) |  t\geq t_0, D_i=1] - E[y_{it}(0) |  t\geq t_0, D_i=1]\\
=& P(y_{it}(1) = 1 | t\geq t_0, D_i=1) - P(y_{it}(0) = 1 | t\geq t_0, D_i=1)\\
=& P(y_{it}^* (1) >0 | t\geq t_0, D_i=1) - P(y_{it}^*(0) >0 | t\geq t_0, D_i=1)\\
=& P(\beta_0 + \beta_1 +\beta_2 +\beta_3 + \alpha_i +u_{it} > 0) - P(\beta_0 + \beta_1 +\beta_2 + \alpha_i +u_{it} > 0)\\
=& P(-\alpha_i -u_{it} < \beta_0 + \beta_1 +\beta_2 +\beta_3) - P(-\alpha_i -u_{it} < \beta_0 + \beta_1 +\beta_2)\\
=& F(\beta_0 + \beta_1 +\beta_2 +\beta_3) - F(\beta_0 + \beta_1 +\beta_2)
\end{align*}


\begin{align*}
trend =& E[y_{it}(0) |  t\geq t_0, D_i=0] - E[y_{it}(0) |  t<t_0, D_i=0]\\
=& P(y_{it}(0)=1 |  t\geq t_0, D_i=0) - P(y_{it}(0)=1 |  t<t_0, D_i=0)\\
=& P(y^*_{it}(0)>0 |  t\geq t_0, D_i=0) - P(y^*_{it}(0)>0 |  t<t_0, D_i=0)\\
=& P(-\alpha_i -u_{it} < \beta_0 +\beta_2) - P(-\alpha_i -u_{it} < \beta_0 )\\
=& F(\beta_0 + \beta_2) - F(\beta_0)
\end{align*}

\begin{align*}
baseline_0 =& E[y_{it}(0) |  t<t_0, D_i=0]\\
=& P(y_{it}(0)=1 |  t< t_0, D_i=0)\\
=& P(y^*_{it}(0)>0 | t<t_0, D_i=0)\\
=& P(-\alpha_i -u_{it} < \beta_0 ) \\
=& F(\beta_0)
\end{align*}

\begin{align*}
baseline_1 =& E[y_{it}(0) |  t<t_0, D_i=1]\\
=& P(y_{it}(0)=1 |  t< t_0, D_i=1)\\
=& P(y^*_{it}(0)>0 | t<t_0, D_i=1)\\
=& P(-\alpha_i -u_{it} < \beta_0 +\beta_1) \\
=& F(\beta_0+\beta_1)
\end{align*}


, Where $F()$ is the CDF of a $N(0, \sigma^2_a + \sigma^2_u)$



## solving for the $\beta$ coefficients

solving for $\beta_0$
\begin{align*}
baseline_0= F(\beta_0) \\
\Leftrightarrow \\
\beta_0 = F^{-1}(baseline_0)
\end{align*}

solving for $\beta_1$
\begin{align*}
baseline_1= F(\beta_0 + \beta_1) \\
\Leftrightarrow \\
\beta_1 = F^{-1}(baseline_1) - \beta_0
\end{align*}

solving for $\beta_2$
\begin{align*}
trend= F(\beta_0 + \beta_2 ) - F(\beta_0) \\
\Leftrightarrow \\
trend + baseline_0 =F( \beta_0 + \beta_2)\\
\Leftrightarrow \\
F^{-1}(trend + baseline_0 ) =\beta_0 + \beta_2\\
\Leftrightarrow \\
\beta_2 = F^{-1}(trend + baseline_0 ) - \beta_0 
\end{align*}

solving for $\beta_3$
\begin{align*}
ATT= F(\beta_0 + \beta_1 +\beta_2 +\beta_3) - F(\beta_0 + \beta_1 +\beta_2)\\
\Leftrightarrow \\
ATT + F(\beta_0 + \beta_1 +\beta_2) = F(\beta_0 + \beta_1 +\beta_2 +\beta_3) \\
\Leftrightarrow \\
F^{-1}(ATT + F(\beta_0 + \beta_1 +\beta_2) )= \beta_0 + \beta_1 +\beta_2 +\beta_3\\
\Leftrightarrow \\
\beta_3 = F^{-1}(ATT + F(\beta_0 + \beta_1 +\beta_2) )- (\beta_0 + \beta_1 +\beta_2)\\
\end{align*}


# DID and functional forms

## initial DID estimates

```{r DIDy, results='hide', warning = FALSE}
DID_y <- quickmontey(n, nobs, years, b0, b1, b2, b3, std_a, std_v, "y")

```

We begin by allowing the outcome to vary between 0 and 1 across time periods.  We see a slight bias in the DID estimates of the ATT when there exist both a time trend and a group difference in the baseline deforestation rates. This result stems from the fact that the DID estimand does not identify the ATT with the nonlinearity introduced in the DGP. The bias generated is on average `r mean(DID_y$did_biases$V1)`, which represents a bias of `r abs(DID_y$prop_ATT)*100` \% of the ATT. 

```{r DIDyplot, results='hide', warning = FALSE}
DID_y$plot

```

```{r DIDy_it, results='hide', warning = FALSE}
DID_yit <- quickmontey(n, nobs, years, b0, b1, b2, b3, std_a, std_v, "y_it")

```

We will now drop pixels in the periods after they first become deforested. The DID estimates are still biased here. It appears that the sign of the bias does not change, while the bias worsens slightly. The bias generated is `r mean(DID_yit$did_biases$V1)`, which represents `r abs(DID_yit$prop_ATT)*100`\% of the ATT. 

```{r DIDy_it plot, results='hide', warning = FALSE}

DID_yit$plot

```

## two-way fixed effects vs. simple DID

We'll now address the use of two-way fixed effects and the dropping of observations in the periods after pixels are first observed as deforested. The magnitude of the bias is smallest using the basic DID. Note that the DID and 2way fixed effects estimates are identical when observations are not dropped. As we show in our proof, in the general case, using 2 way fixed effects dropping the observations yields the ATT plus the difference between the treated and untreated groups. The DID on the other hand should identify the ATT. 

```{r FE, results='hide', warning = FALSE}
twowayFE <- binary_coeffdist_fcn(n, nobs, years, b0, b1, b2, b3, std_a, std_v)
twowayFE$plot
```

## Two way FE proof

We can show that in the case where the binary outcome is dropped in periods after the outcome is realized as a 1, two-way fixed effects regressions typically do not identify the ATT, but the ATT + the group difference. 

pf:

Consider the regression
$$ y_{it}=\alpha+\eta d2_t+\tau w_{it}+c_i+u_{it}, \text{ for } t=1,2 $$

$y_{it}$ is the binary outcome; $w_{it}$ is a dummy equal to 1 when unit $i$ is treated in time $t$; $d2_t$ is a dummy for the second time period; $c_i$ is an observed effect for unit i.

Jones and Lewis (2015) advise to drop unit $i$ in periods $t+1,..., T$ when $y_{it}=1$.

In the two period case, we have 
$$ y_{i1}=\alpha+\tau w_{i1}+c_i+u_{i1}$$
\[y_{i2}= \begin{cases} 
      \alpha+\eta d2_2+\tau w_{i2}+c_i+u_{i2} & y_{i1}= 0\\
      NAN & y_{i1} \neq 0 
   \end{cases}
\]


First differencing,

\[y_{i2}-y_{i1}= \begin{cases} 
  \alpha+\eta d2_2+\tau w_{i2}+c_i+u_{i2} - \alpha-\tau w_{i1}-c_i-u_{i1} & y_{i1}= 0\\
      NAN & y_{i1} \neq 0 
   \end{cases}
\]

 Focusing on the first case, where $y_{i1}=0$
\begin{align*}
y_{i2}-y_{i1}&=\alpha+\eta d2_2+\tau w_{i2}+c_i+u_{i2} - \alpha-\tau w_{i1}-c_i-u_{i1}\\
&= \alpha+\eta d2_2+\tau w_{i2}+c_i+u_{i2} - \alpha-c_i-u_{i1}\\
&= \eta d2_2+\tau w_{i2}+u_{i2}  -\tau w_{i1} -u_{i1}\\
&=\eta +\tau w_{i2}+\Delta u_i
\end{align*}

Coming back to the general expression,

\[y_{i2}-y_{i1}= \begin{cases} 
 \eta +\tau w_{i2}+\Delta u_i & y_{i1}= 0\\
      NAN & y_{i1} \neq 0 
   \end{cases}
\]

(NAN - $(\alpha+\tau w_{i1}+c_i+u_{i1})$ is NAN)

We have that with binary $w_{i2}$
\begin{align*}
\hat{\tau}&= \frac{1}{n_{ST}}\sum_{i:w_i=1} y_{i2} - \frac{1}{n_{ST}}\sum_{i:w_i=1} y_{i1} - (\frac{1}{n_{SU}}\sum_{i:w_i=0} y_{i2} - \frac{1}{n_{SU}}\sum_{i:w_i=0} y_{i1})
\end{align*}

, where $n_{ST}$ and $n_{SU}$ are the number of surviving treated and untreated units such that $y_{i1}=0$, respectively. 

Note: Since this captures where $y_{i1}=0$, we have $\frac{1}{n_{ST}}\sum_{i:w_i=1} y_{i1}=0$ and $\frac{1}{n_{SU}}\sum_{i:w_i=0} y_{i1}=0$ 

Then,   

\begin{align*}
\hat{\tau}&= \frac{1}{n_{ST}}\sum_{i:w_i=1} y_{i2} - 0 - (\frac{1}{n_{SU}}\sum_{i:w_i=0} y_{i2} - 0)\\
&= \frac{1}{n_{ST}}\sum_{i:w_i=1} y_{i2}(1) - \frac{1}{n_{SU}}\sum_{i:w_i=0} y_{i2}(0)\\
 &(\text{whether we see treated or untreated outcome})\\
&=    \frac{1}{n_{ST}}\sum_{i:w_i=1} y_{i2}(1) - y_{i2}(0)  \\
+& \frac{1}{n_{ST}}\sum_{i:w_i=1}y_{i2}(0) - \frac{1}{n_{SU}}\sum_{i:w_i=0} y_{i2}(0) \\
&(\text{adding and subtracting $\frac{1}{n_{ST}}\sum_{i:w_i=1}y_{i2}(0)$})\\
&= ATT+Diff\\
\blacksquare &
\end{align*}




## functional form test (logit, probit, vs. DID)


```{r functionalforms, results='hide', warning = FALSE}
functionalform <- funcform(n, nobs, years, b0, b1, b2, b3, std_a, std_v)
functionalform$plot
```

The above plot shows the DID bias depending on functional form decisions. 


# Aggregating pixels

## various outcomes wheen aggregating

We now aggregate to the grid level. Below, we see the bias introduced by using different outcome variables and specifications commonly used in the literature. (Need a table here to show the different outcomes and will want to cite different papers that use these outcomes)

\begin{center}
\begin{tabular}{ |c|c|c|c|c| } 
 \hline
 number & outcome & additional covariates & model type & paper \\ 
 \hline
 1 & $\frac{F_{it-1} - F_{it}}{F_{it-1}} $ & & Two-way FE& Busch et al. 2015; Carlson et al. 2018 \\ 
 2 & $\frac{F_{i0} - F_{it}}{F_{i0}} $ &  & Two-way FE & Pfaff 1999
\\ 
 3 & $\frac{F_{it-1} - F_{it}}{F_{i0}} $& & Two-way FE & \\ 
 4 & $F_{it}$ & $F_{it-1}$ & Two-way FE & \\
 \hline
\end{tabular}
\end{center}

```{r outcomes, results='hide', warning = FALSE}
outcomes <- outcome_comparison(n, nobs, years, b0, b1, b2, b3, std_a, std_v, cellsize)
outcomes$plot
```


## aggregating to county, property, and grid levels
 
Moving forward, we use the first outcome from above, as it generated the least bias. We consider three possible aggregation methods: county, property, and grid level in order to look at the distribution of the estimates. At this point we also introduce property level perturbations into the DGP. 

```{r generating parameters with std_p, results = 'hide', warning=FALSE}

std_avp = (std_a^2 + std_v^2 + std_p)^.5
b0 = qnorm(base_0, mean = 0, sd = std_avp)
b1 = qnorm(base_1, mean = 0, sd = std_avp) - b0
b2 = qnorm(trend + base_0, mean = 0, sd = std_avp) - b0
b3 = qnorm( pnorm(b0+b1+b2, mean = 0, sd = std_avp) + ATT , mean = 0, sd = std_avp) - (b0 + b1 + b2)

```



```{r aggregate, results = 'hide', warning=FALSE}
aggregation <- aggregation_method(n, nobs, years, b0, b1, b2, b3, std_a, std_v, std_p, cellsize, ppoints, cpoints)
aggregation$plot

```


## Weighting the regression

Interestingly, we note that upon weighting the regression aggregated to the property level, the distribution is significantly wider. This also appears to happen with the regression aggregated to the county level, but not nearly to the same extent. Weighting the regression aggregated to the grid level has almost no impact. This makes intuitive sense, since the grids are all of equal size. We see a larger impact of weighting the regressions in the property and county cases, because there is more area heterogeneity across units. 

```{r weighting, results='hide', warning=FALSE}
weighting <- weightingarea(n, nobs, years, b0, b1, b2, b3, std_a, std_v, std_p, cellsize, ppoints, cpoints)
weighting$plot
```


## Coverage based on aggregation and standard error

We consider three possible aggregation methods: county, property, and grid level. We then compute the coverage probability of the ATT with a 95% CI. We first cluster standard errors at the group level and then, simply use White standard errors for comparison.

Property level aggregation also leads to roughly the expected coverage, while the county and grid level aggregation results in under coverage. It is unclear how much of the issue is related to the bias rather than the standard errors themselves. The coverage seems to be the same whether we cluster the standard errors at the group level or simply use White standard errors. 



\begin{center}
\begin{tabular}{ |c|c|c| } 
 \hline
 level of aggregation & std. errors & coverage probability\\ 
 \hline
 grid & clustered at grid & `r weighting$grid_clustercover` \\
 property & clustered at property & `r weighting$prop_clustercover` \\
 county & clustered at county & `r weighting$county_clustercover` \\
 grid & classical & `r weighting$grid_cover` \\
 property & classical & `r weighting$prop_cover` \\
 weighted county & classical & `r weighting$county_cover` \\
 weighted grid & classical & `r weighting$wgrid_cover` \\
 weighted property & classical & `r weighting$wprop_cover` \\
 weighted county & classical & `r weighting$wcounty_cover` \\
 \hline
\end{tabular}
\end{center}


```{r xy plot ATT, results = 'hide', warning=FALSE}
min_ATT = -.12
max_ATT = .12

ATT_bias <- xy_ATT(50, nobs, years, min_ATT, max_ATT, base_0, base_1, trend, std_a, std_v, "y_it")


ATT_bias$plot

  
```
