---
title: "outline_sep19"
author: "Alberto Garcia"
date: "September 3, 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
source('quickmontey.R')
source('binary_coeffdist_fcn.R')
source('funcform.R')
source('grid_scapegen.R')
source('grid_sim.R')
source('property_scapegen.R')
source('weightedreg_test.R')
source('property_perturb.R')
source('outcome_comparison.R')
source('county_scapegen.R')
source('county_sims.R')
source('clustercover.R')

```

Define universal parameters
```{r parameters}
b0 = .05
b1 = -0.1
b2 = -0.2
b3 = .16
std_a = .1
std_v = 0.25
years = 3
nobs = 10000
n = 100


cellsize = 20
ppoints = 100
std_p = .2

cpoints = 20
```



```{r DIDy, results='hide'}
DID_y <- quickmontey(n, nobs, years, b0, b1, b2, b3, std_a, std_v, "y")
DID_y$plot
```

Above, we see a slight bias in the DID estimates of the ATT when the binary outcome is able to vary between 0 and 1 across periods. This comes from the DID estimand not identifying the ATT with the DGP. 




```{r DIDy_it, results='hide'}
DID_yit <- quickmontey(n, nobs, years, b0, b1, b2, b3, std_a, std_v, "y_it")
DID_yit$plot
```

In the above plot, we drop pixels in the periods after they first become deforested. The DID estimates are still biased here. It appears that neither the sign nor magnitude of the bias seems to change. 


```{r FE, results='hide'}
twowayFE <- binary_coeffdist_fcn(n, nobs, years, b0, b1, b2, b3, std_a, std_v)
twowayFE$plot
```

Above, we address the use of two-way fixed effects and the dropping of observations in the periods after pixels are first observed as deforested. The magnitude of the bias is smallest using the basic DID. The bias is negative when using the 2way FE dropping deforested obs and when using either the DID or 2way FE keeping obs. Note that the DID and 2way fixed effects estimates are identical when observations are not dropped. As we show in our proof, in the general case, using 2 way fixed effects dropping the observations yields the ATT plus the difference between the treated and untreated groups. The DID on the other hand should identify the ATT. 




```{r functionalforms, results='hide'}
functionalform <- funcform(n, nobs, years, b0, b1, b2, b3, std_a, std_v)
functionalform$plot
```

The above plot shows the DID bias depending on functional form decisions. Interestingly in our case, the magnitude of the bias is similar using each of the three method. The sign does differ, however. 

```{r outcomes, results='hide'}
outcomes <- outcome_comparison(n, nobs, years, b0, b1, b2, b3, std_a, std_v, cellsize)
outcomes$plot
```

We now aggregate to the grid level. Above, we see the bias introduced by using different outcome variables. (Need a table here to show the different outcomes and will want to cite different papers that use these outcomes. Currently only two outcomes being used in this function)




```{r property_perturbations, results='hide'}
perturb <- property_perturb(n, nobs, years, b0, b1, b2, b3, std_a, std_v, std_p, cellsize, ppoints)
perturb$plot
```

In order to compare property level and grid level aggregation, we introduce perturbations at the property level to see if this induces any differences. It appears that when property level perturbations are introduced, aggregating to the property level and using property level fixed effects rather than doing so at the grid level may improve the estimates.

What is now interesting is that we seem to be identifying the ATT rather than the DID estimand that would be expected. In either case, aggregating to properties results in less bias relative to either teh DID or ATT parameters. In order to see if this is an anomaly, I alter the starting parameters 



```{r property_perturbations_2, results='hide'}
perturb2 <- property_perturb(n, nobs, years, -b0, -b1, b2, -b3, std_a, std_v, std_p, cellsize, ppoints)
perturb2$plot
```

It seems that the property level aggregation does a better job of identifying the DID estimand and ATT here. 



The bias is smallest relative to both the ATT and DID estimand when we aggregate to the property level. It appears that the distribution of the estimates is much wider when aggregating to properties, however. In the next chunk, we look at the coverage of each of the aggregation decisions. 


```{r aggregate, results = 'hide'}
countycoverage <- clustercover(n, nobs, years, b0, b1, b2, b3, std_a, std_v, std_p, 5, ppoints, cpoints)
countycoverage$plot
```

```{r coverages}

#coverages by aggregation nmethod
print(countycoverage$grid_cover)
print(countycoverage$prop_cover)
print(countycoverage$county_cover)

```





```{r weighting, results='hide'}
weighting <- weightedreg_test(n, nobs, years, b0, b1, b2, b3, std_a, std_v, std_p, cellsize, ppoints)
weighting$plot
```